{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb410052",
   "metadata": {},
   "source": [
    "## Dynamic Parameters: `kwargs`\n",
    "\n",
    "The `kwargs` parameter lets you dynamically update any model configuration at runtime.  \n",
    "It works whether you are using default or custom settings, and it overrides existing settings or adds new ones.  \n",
    "\n",
    "It supports both parsed (structured output) and unparsed modes.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eebb80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azllm import azLLM\n",
    "manager = azLLM()  # Instantiated with default parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece88a22",
   "metadata": {},
   "source": [
    "### Format Guide by Method:\n",
    "\n",
    "**`generate_text`**:  \n",
    "\n",
    "- Accepts a **single dictionary**\n",
    "\n",
    "Exmaple: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9367487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am OpenAI. The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the capital of France?\"\n",
    "generated = manager.generate_text(\n",
    "    'openai',\n",
    "    prompt,\n",
    "    kwargs={\n",
    "        'temperature': 1,\n",
    "        'system_message': 'You are an advanced AI assistant. Always start you answer with `Hi, I am OpenAI`'\n",
    "    },\n",
    ")\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47701d35",
   "metadata": {},
   "source": [
    "**`batch_generate`**:  \n",
    "- Accepts a **list of dictionaries**, one per prompt.  \n",
    "  > Tip: If a prompt doesn’t require custom kwargs, include an **empty dictionary** in its place.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efeca352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I am anthropic\n",
      "\n",
      "The capital of France is Paris. Paris is located in the north\n",
      "________________________________________________________________________________\n",
      "Hahaha! Sure, I'd love to tell you a joke!\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "Because they make up everything!\n",
      "\n",
      "*laughs* Hope that brightened your day a little!\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_prompts = [\n",
    "    'What is the capital of France?',\n",
    "    'Tell me a joke.'\n",
    "    ]\n",
    "\n",
    "results = manager.batch_generate('anthropic', batch_prompts, \n",
    "                                    kwargs = [{'max_tokens': 20, 'system_message': 'Always start your answer with `Hi, I am anthropic`' },\n",
    "                                               {'temperature': 1, 'system_message': 'You are a creative assistant who laughs before answer.'}],\n",
    "                                   )\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print('_'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4877ccd",
   "metadata": {},
   "source": [
    "**`parallel_generate`**:  \n",
    "\n",
    "- Accepts a **list of dictionaries**, one per model.  \n",
    "\n",
    "  > Tip: Same as above—use **empty dictionaries** if needed.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e13a54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: grok:1,  Result: Yes, the capital of France is Paris.\n",
      "\n",
      "Model: openai:0,  Result: No.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is the capital of France?'\n",
    "models = [\n",
    "    'openai',\n",
    "    'grok',]\n",
    "\n",
    "results = manager.generate_parallel(prompt, models,\n",
    "                                     kwargs= [{'temperature': 0.5, 'system_message': 'You are an assistant who says `no` to every question'}, \n",
    "                                              {'system_message': 'You are an assistant who says `yes` to every question'},\n",
    "                                            ])\n",
    "for model, result in results.items():\n",
    "    print(f\"Model: {model},  Result: {result}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azllm-I_5BKfWE-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
